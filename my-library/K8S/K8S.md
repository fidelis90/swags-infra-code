# Kubernetes 

## Kubernetes Architecture

- Consists of nodes 
  - Master 
  - Worker 

- When you install k8s on a system, you get the following components

  - kubelet service 
    - The service running on the node, making sure the container is running as expected 

  - api server 
    - The frontend ( user facing component)
    - the kubectl, eksctl and other cli tools for k8s communicate with this component 

  - etcd 
    - The brain of the cluster, as it stores information about the cluster in a key: value format 
    - The App data isn't stored in etcd

  - scheduler 
    - schedules pods on nodes 

  - controller 
    - The actual control plane, that controls the orchestration activities in your cluster 

  - container runtime 
    - Underlying container engine (Docker)

## Processes in Master Nodes 

- Api server 
- kubelet 
- etcd 
- controller 
- scheduler 
- container runtime 

## Processes in Worker nodes 

- Container runtime 
- kubelet 
- kubeproxy 

## Kubectl 

- The command line utility used to create and communicate with the cluster 
- Common kubectl commands 
  - kubectl run hello-minikube 
  - kubectl cluster-info 
  - kubectl get nodes 
  -  kubectl --help 

## KubeProxy

- This must be on the worker node 
- Helps pod service to know which replica of pod to send traffic to in order to get fast response. 

## Pod 

- Smallest unit of k8s 
- Abstraction over container layer 
- usually 1 container per pod 
- each pod gets an IP 
- Pod can die easily and k8s deploys a new pod immediately which gets a new IP 
- so in order to avoid loss of communication btw multiple pods in a node, we use #SERVICES 

## Service and Ingress 

- static/permanent IP attached to a pod 
- each pod get a service attached 
- lifecycle of pod and service are not connected 
- even when a pod dies, the service still exists
- pods communicate with each other using service 

### Ingress 

- we use ingress for loadbalancers or public facing servers 
- requests goes to ingress first, then ingress sends it to service 


## ConfigMap and Secrets 

- ConfigMap holds external conf of your application 
  - e.g url 

- Secrets holds conf of your app 
  - passwords, certificates 
  - base64 encoded 

- we use the values of data in ConfigMap and Secrets in our pod using environment variables 

## Data Storage 

- For persistent data, we need to attach an external volume 
- k8s doesn't handle data consistency 

## Deployment 

- blueprints for creating the orchestration configuration 
- we work with deployments and k8s creates the pods on the nodes 
- we can't use deployments for database pods, why because of avoiding data inconsistencies between the various pods deployed from deployment 

## StatefulSet 

- for stateful apps like databases 
- this is a deployment specifically for apps like db so that data is syncd btw nodes 
- it is hard to use stateful sets in k8s 
- it is best practice to host database outside of the k8s cluster 

## Minikube and Kubectl 

- In a production setup, we have multiple master and worker nodes 

- For test on a local machine, we use minikube 
 - all master and node processes running on one local machine 

- The processes in the local machine 
  - docker 
  - minikube creates a virtualbox on your laptop 
  - node runs in the virtualbox 
  - 1 node k8s cluster

- aFTER Creating 1 node k8s cluster ( minikube )
- we use kubectl to read and write to the k8s cluster 
- kubectl is used in any type of k8s cluster 
  - eks 
  - aks 
  - gks

## Minikube Setup 

- Virtualization on your machine is needed 

# Layers Of Abstraction 

Deployment manages 
Replicaset manages 
Pod is an abstraction of
Container 

- In practice, we use configuration files for k8s 
- We use kubectl apply -f config.yaml 

# Syntax of the Kubernetes Config file 

- Each configuration file has 3 parts 
  - metadata 
    - This has labels 
  - spec
    - this is specific to the kind
    - selector  
  - status 
    - automatically generated by k8s 

# Simple Kubernetes Project 

- webapp: MongoExpress 
- External service for MongoExpress
- database: MongoDB 
- internal service for mongoDB 
- Config map: holds mongoDB url 
- secret: hold mongoDB username and password 
- 

# Namespace 

- we use namespace to organize resources 
- virtual cluster in a cluster 
- 4 namespaces per default 
  - default
    - resources you create are kept here 
  - kube-system 
    - do not create or modify kube system 
  - kube-public 
    - publicly accessible data 
  - kube-node-lease

- kubectl create ns my-namespace 
- you can create ns using config file 
- it will be inside the metadata block 

- Command to get Components that can/can't be created in a namespace
  kubectl api-resources --namespaced=true
  kubectl api-resources --namespaced=false

- Resources you can't share within namespaces 
  - configMaps
  - secrets 

- Resource you can reference within ns 
  - service 

- Install kubens on mac using "brew install kubectx" 
- command is kubens 

## External service vs Ingress 

- To be able to access the app from the public/externally using the ip of the node and port 
  - we use external service 
  - we use this for test case mostly 

- Ingress 
  - Not a service, just another way of routing traffic to your service 
  - it is a collection of routing rules that governs how external users access services  running inside a k8s cluster 
- Ingress is the most useful if you want to expose multiple services under the same IP 


NodePort

NodePort is a Kubernetes service type that listens on a port on the node and forward requests on that port to a pod on the node. Let's look at an example.

    We have a node with IP address 10.1.3.4.
    The internal pod network of the node is in the range 10.244.0.0
    The pod itself has an IP of 10.244.0.2.
    The actual web server is running on port 80 in the pod. 

- targetPort: The port on the pod where the actual web server is running, 
- nodePort: The port on the node which is used to access the web server externally.
- The ‘port’ is the port exposed to the NodePort service itself

- For ingress to work, we need to have a dedicated pod for ingress-controller to help manage the redirections for ingress. e.g nginx-ingress-controller 

# FLOW 

http://myapp.com > ALB > ingress-controller > ingress > internal-svc > my-app-pod 

INSTALL INGRESS CONTROLLER IN MINIKUBE 

$ minikube addons enable ingress 
$ minikube service <name of service>

# HELM 

- a package manager for k8s 
- for example if you need elastic stack for logging in your cluster, we can use helm charts to create the services 
- helm charts are bundle of yaml files 
- you can use helm to create your own helm charts and push them to helm repo 
- download and use exisiting ones 
- Example of deployments we use helm charts for 
  - Monitoring tools like Prometheus 
  - Database apps like: MongoDB, ElasticSearch, MySQL 
- helm can be used as a templating engine 
- you can build your microservices in a helm chart 

## HELM CHART STRUCTURE 

- mychart: the folder ( name of chart )
- chart.yml: metadata info about the chart 
- value.yml: values for the template files 
- charts: the folder that holds chart dependencies ( if the chart depends on other charts )
- templates: the folder that holds the actual template files 

- helm install <chartname>
- helm install --value=my-values.yml <chart-name>

- helm helps with release management 


# Kubernetes volume 

- The storage must be available on all nodes 
- storage must be highly available even if cluster crashes 
- an external plugin to your cluster 
- Persistent volume 
   - using yaml file to create the storage 
   - specifies the storage backend (physical storage)
   - they are not namespaced 
- Persistent Volume creates the Volume itself 
- Persistent volume claim, claims the volume from the PV 
  - Claims must exist in the same namespace as the deployment using the claim 
- The deployment uses the PV by referencing the PVc 

- The administrator creates the storage resource (PV)
- The k8s user creates the PV claim 

## Local Volumes 

- configmap and secrets 
- they are local volumes 
- managed by k8s 
- config file for your pod 
- mount them into  your pod 

##  Storage class 

- this is another abstract that creates pv dynamically 
- kubernetes uses the provisioner module to specify which platform the storage will be used to create the storage 
- the storage class will be referenced in the pvc config using the storageClassName module 

# Stateful sets 

- stateful apps like db apps ( apps that stores data )
- these apps are deployed using Statefulset 

## Deployments vs StatefulSet 

- replicating stateful apps is more difficult, needs other requirements 
- creates a sticky ID for each pod 
- stateful apps are not perfect for containerized environment 


## Kubernetes Services Explained 

- From the K8s architecture 
  - node Pod Container 
  - The node is on the same network with my windows laptop cos it's a VM e.g 192.168.1.10
  - The Pod is on the kubernetes network: 10.244.0.2 
  - for this reason i can't communicate with the pod outside the cluster 
  - communications will be only within the pods  
  - i can only ssh into the node; then curl http://10.244.0.2 
  - we need to be able to communicate with the pod from outside the node (cluster)

- This is where service comes to play 
  - This allows us to use the IP address of the node and communicate to the pods by opening a port 
  - The service listens to traffic on a port on the node and forwards the traffic to a port on the pod 
  - The port it sends the traffic to on the pod is called targetPort 
  - the port it listens to on the node is called the nodePort 
  - this type of service is called nodePort cos it listens to a port on the node 
  - there are different types of services 

- Nodeport
  - there are 3 pods involved 
    - port on the pod where the app is running: 80 = targetPort
    - port on the service: 80 = port 
       - The service has its own IP called the clusterIP 
    - port on the node: 30008 = nodePort 
  - listens to a port on the node and forwards traffic from that port to the port on the pod 
  - nodeport is tied to just one pod 

- Loadbalancer 
  - same virtual server within the cluster, has it's own IP address 
  - Now this has inbuilt loadbalancer and can open same nodeport on all nodes in the cluster and user can access 
  the pods through either of the node's IP using same port number opened.

- ClusterIP 
  - same virtual server within the cluster, has it's own IP address
  - just that no port is opened on the node, so the pods ain't accessible outside the cluster 

# Kubernetes Ingress 

- assuming i have an app in a pod and i want the app to be accessed using www.swags-app.com 
- Current setup 
  - swags-app pod inside a node 
  - also a mysql pod inside the node 
  - mysql-svc to allow communication btw the swags-app pod and the mysql pod 
    - this is done using configmap 
    - we reference the mysql service in the swags-app pod 
  - we create a nodeport service for the swags-app, so the application is accessible from the outside world 
    using the ip-of-node:30008 
  - so we have a dns-server and we map the node-ip to the domain-name www.swags-app.com
  - http://swags-app.com:30008 
  - then we have a proxy server in front of the node to route traffic from port 80 to the 30008 
  - point dns record to the proxy server 
  - now we can have users: http://swags-app.com
  - Now the above setup is best for an app hosted on prem. 

- For app hosted in public cloud platform 
  - we create a loadbalancer service instead of nodePort 
  - cos by creating a loadbalancer service, kubernetes sends request to AWS to create a NLB 
  - The NLB acts as the proxy server that routes traffic from port 80 to the nodePorts 
  - we just map www.swags-app.com:80 -> <PublicIP-OF-NLB>

- Now the company is growing and we want to add a new service to the swags-app application 
  - www.swags-app.com/wears
  - www.swags-app.com/watch 
  - Curent setup 
    - we create the watch service and pods in the same cluster 
    - create watch loadbalancer service, which creates another NLB 
    - now we will be needing another loadbalancer to configure rules on, which gets complicated as the application scales 

- The solution to the problem is Ingress 
  - it helps manage all within the k8s cluster 
    - the NLBs 
    - the ALBs 
    - the SSL/TLS encryption (https)
  
  - Ingress sits infront of all the front-end services 
    - makes it easy to use just one url to communicate with all the pods in the cluster based on rules specified in the ingress 
    - in order to access the ingress from outside the cluster, ingress needs a ingress controller 
    - Types of ingress controller 
      - nginx ( maintained by k8s ) ( in min)
      - Contour 
      - aws-loabalancer-controller etc

    - the ingress controller is deployed into the cluster as seperate deployment 
      - for easy deployments of ingress-controller, we can use helm 

    - the service attached to our pods should be internal(clusterIP), since we using the ingress 

# AWS Loadbalancer controller 

- a controller to help manage elastic load balancers for a k8s cluster 
- formerly known as aws alb ingress controller 
- when you install the aws loadbalancer controller, the controller dynamically provisions 
  - AWS ALB when you create the ingress based on the annotations added in the ingress resource 
    - the target groups are created for each backend specified in the ingress resource 
    - the alb url is accessed with the path or query params 
    - based on the rules configured in the ingress resource, the request is redirected to a specific target group and reaching pod service using clusterip or nodeport 
  - AWS NLB when you create the loadbalancer service type 





















































































